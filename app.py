# ========== app.py ==========

import streamlit as st
import joblib
import re
from collections import Counter
import matplotlib.pyplot as plt

# ุชุญููู ุงูููุงุฐุฌ ุงููุญููุธุฉ
tfidf = joblib.load('tfidf_vectorizer.pkl')
label_encoder = joblib.load('label_encoder.pkl')
svm_model = joblib.load('svm_model.pkl')

# ุฏุงูุฉ ุชูุธูู ุงููุตูุต
def clean_text(text):
    text = re.sub(r'[^\u0600-\u06FF\s]', '', text)  # ุฅุจูุงุก ุงูุฃุญุฑู ุงูุนุฑุจูุฉ ููุท
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# ุฏุงูุฉ ุงูุชูุจุค ุจุฃุนูู 3 ุชุตูููุงุช
def predict_top3(text):
    cleaned = clean_text(text)
    vectorized = tfidf.transform([cleaned])
    probabilities = svm_model.decision_function(vectorized)

    # ุฅุฐุง ูุงู ุงููููุฐุฌ ูุง ูุฏุนู decision_function (ุจุนุถ ุงูููุงุฐุฌ)ุ ุงุณุชุฎุฏู predict_proba
    if len(probabilities.shape) == 1:
        probabilities = probabilities.reshape(1, -1)

    top3_idx = probabilities.argsort()[0][-3:][::-1]
    top3_probs = probabilities[0][top3_idx]
    top3_labels = label_encoder.inverse_transform(top3_idx)

    # ุชุทุจูุน ุงูููู ุฅูู ูุณุจ ูุฆููุฉ
    normalized_scores = (top3_probs - top3_probs.min()) / (top3_probs.max() - top3_probs.min() + 1e-6)
    percentages = (normalized_scores * 100).astype(int)

    return list(zip(top3_labels, percentages))

# ุฏุงูุฉ ุงูุชุญููู ุงูุฅุญุตุงุฆู ูููุต
def analyze_text(text):
    words = clean_text(text).split()
    num_words = len(words)
    most_common = Counter(words).most_common(5)
    return num_words, most_common

# ุฏุงูุฉ ุนุฑุถ ุตูุญุฉ ุญูู ุงููุดุฑูุน
def show_about():
    st.markdown("""
    ## ุญูู ุงููุดุฑูุน
    ูุฐุง ุงููุดุฑูุน ููุฏู ูุธุงููุง ุฐูููุง ูุชุตููู ุงูุฃุฎุจุงุฑ ุงูุนุฑุจูุฉ ุฅูู ูุฆุงุช ูุญุฏุฏุฉ ุจุงุณุชุฎุฏุงู ุชูููุงุช ูุนุงูุฌุฉ ุงููุบุฉ ุงูุทุจูุนูุฉ (NLP) ูุฎูุงุฑุฒููุงุช ุชุนูู ุงูุขูุฉ (SVM).
    
    - **ูุฌููุนุฉ ุงูุจูุงูุงุช**: SANAD Dataset
    - **ุงูููุงุฐุฌ ุงููุณุชุฎุฏูุฉ**: TF-IDF + SVM
    - **ุฃุนูู 3 ุงุญุชูุงูุงุช**: ุงููุธุงู ูุนุฑุถ ูู ุซูุงุซ ุงุญุชูุงูุงุช ูุน ูุณุจุฉ ุงูุซูุฉ ููู ูุฆุฉ
    - **ุชุญููู ูุตู ุจุณูุท**: ุนุฑุถ ุนุฏุฏ ุงููููุงุช ูุฃูุซุฑ ุงููููุงุช ุชูุฑุงุฑุงู
    """)

# ุฅุนุฏุงุฏ ุงูุตูุญุฉ
st.set_page_config(page_title="ุชุตููู ุงูุฃุฎุจุงุฑ ุงูุนุฑุจูุฉ", layout="centered", page_icon="๐ฐ")

# ุงูุนููุงู ุงูุฑุฆูุณู
st.title("๐ฐ ุชุตููู ุงูุฃุฎุจุงุฑ ุงูุนุฑุจูุฉ ุจุงุณุชุฎุฏุงู ุงูุฐูุงุก ุงูุงุตุทูุงุนู")

# ุชุจููุจุงุช
tabs = st.tabs(["ุชุตููู ููุงู", "ุญูู ุงููุดุฑูุน"])

# ุงูุชุจููุจ ุงูุฃูู: ุชุตููู ููุงู
with tabs[0]:
    st.subheader("ุงุฏุฎู ูุต ุงูููุงู ุฃุฏูุงู:")
    user_input = st.text_area("โ๏ธ ุฃุฏุฎู ุฃู ุงูุตู ูุต ุงูููุงู ููุง:", height=250)

    if st.button("๐ ุชุตููู ุงูููุงู"):
        if user_input.strip() == "":
            st.warning("โ๏ธ ุงูุฑุฌุงุก ุฅุฏุฎุงู ูุต ูุจู ุงูุชุตููู.")
        else:
            # ุงูุชูุจุค
            top3_predictions = predict_top3(user_input)
            
            # ุนุฑุถ ุงููุชุงุฆุฌ
            st.success("โ ุฃุนูู 3 ุงุญุชูุงูุงุช ููุชุตููู:")
            for label, percent in top3_predictions:
                st.write(f"๐น {label} - {percent}%")

            # ุชุญููู ุงููุต
            st.markdown("---")
            st.info("๐ ุชุญููู ุงูููุงู:")
            num_words, most_common_words = analyze_text(user_input)
            st.write(f"- ุนุฏุฏ ุงููููุงุช: {num_words}")
            st.write("- ุฃูุซุฑ ุงููููุงุช ุชูุฑุงุฑุงู:")
            for word, count in most_common_words:
                st.write(f"  โข {word} ({count} ูุฑุงุช)")

# ุงูุชุจููุจ ุงูุซุงูู: ุญูู ุงููุดุฑูุน
with tabs[1]:
    show_about()

# ุชุฐููู ุงูุตูุญุฉ
st.markdown("---")
st.caption("ุชู ุชุทููุฑู ุจูุงุณุทุฉ ุงูุฐูุงุก ุงูุงุตุทูุงุนู 2025 ยฉ")

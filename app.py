# ========== app.py ==========

import streamlit as st
import joblib
import re
from collections import Counter
import matplotlib.pyplot as plt
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer

# ========== ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨Ø© ==========
tfidf = joblib.load('tfidf_vectorizer.pkl')
label_encoder = joblib.load('label_encoder.pkl')
svm_model = joblib.load('svm_model.pkl')

# ========== Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© ==========

# ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
def clean_text(text):
    text = re.sub(r'[^\u0600-\u06FF\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø£Ø¹Ù„Ù‰ 3 ØªØµÙ†ÙŠÙØ§Øª
def predict_top3(text):
    cleaned = clean_text(text)
    vectorized = tfidf.transform([cleaned])

    try:
        probabilities = svm_model.decision_function(vectorized)
    except:
        probabilities = svm_model.predict_proba(vectorized)

    if len(probabilities.shape) == 1:
        probabilities = probabilities.reshape(1, -1)

    top3_idx = probabilities.argsort()[0][-3:][::-1]
    top3_scores = probabilities[0][top3_idx]
    top3_labels = label_encoder.inverse_transform(top3_idx)

    normalized_scores = (top3_scores - top3_scores.min()) / (top3_scores.max() - top3_scores.min() + 1e-6)
    percentages = (normalized_scores * 100).astype(int)

    return list(zip(top3_labels, percentages))

# ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ (Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª + Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©)
def analyze_text(text):
    words = clean_text(text).split()
    num_words = len(words)
    most_common = Counter(words).most_common(5)
    return num_words, most_common

# ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ
def summarize_text(text, sentence_count=3):
    parser = PlaintextParser.from_string(text, Tokenizer("arabic"))
    summarizer = LsaSummarizer()
    summary = summarizer(parser.document, sentence_count)
    summarized_text = " ".join(str(sentence) for sentence in summary)
    return summarized_text

# ØµÙØ­Ø© Ø­ÙˆÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
def show_about():
    st.markdown("""
    ## Ø­ÙˆÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ğŸ§ 
    Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠÙ‚ÙˆÙ… Ø¨ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ø¥Ø®Ø¨Ø§Ø±ÙŠØ© Ø§Ù„Ù…ÙƒØªÙˆØ¨Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¥Ù„Ù‰ ÙØ¦Ø§Øª Ù…Ø­Ø¯Ø¯Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.
    
    - **Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª**: SANAD Dataset.
    - **Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ù†ØµÙŠ**: TF-IDF Vectorization.
    - **Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…**: Support Vector Machine (SVM).
    - **Ù…ÙŠØ²Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©**: Ø¹Ø±Ø¶ Ø£ÙØ¶Ù„ 3 ØªØµÙ†ÙŠÙØ§ØªØŒ Ù†Ø³Ø¨Ø© Ø§Ù„Ø«Ù‚Ø©ØŒ ØªØ­Ù„ÙŠÙ„ Ù†ØµÙŠØŒ ÙˆØªÙ„Ø®ÙŠØµ Ø§Ù„Ù…Ù‚Ø§Ù„ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹.
    
    ### Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø·Ø§Ù„Ø¨:
    Ù…Ø´Ø±ÙˆØ¹ Ù„Ù…Ù‚Ø±Ø± EMAI 631 â€“ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© (NLP).
    """)

# ========== Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØ§Ø¬Ù‡Ø© Streamlit ==========

st.set_page_config(
    page_title="ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ",
    page_icon="ğŸ“°",
    layout="centered"
)

# ÙˆØ§Ø¬Ù‡Ø© RTL
st.markdown(
    """
    <style>
    body, .stApp {
        direction: rtl;
        text-align: right;
    }
    </style>
    """,
    unsafe_allow_html=True
)

st.title("ğŸ“° ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ")

tabs = st.tabs(["ğŸ“° ØªØµÙ†ÙŠÙ Ù…Ù‚Ø§Ù„", "â„¹ï¸ Ø­ÙˆÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"])

# ======== Ø§Ù„ØªØ¨ÙˆÙŠØ¨ Ø§Ù„Ø£ÙˆÙ„: ØªØµÙ†ÙŠÙ Ù…Ù‚Ø§Ù„ ========
with tabs[0]:
    st.subheader("ğŸ“„ Ø£Ø¯Ø®Ù„ Ù†Øµ Ø§Ù„Ù…Ù‚Ø§Ù„:")
    user_input = st.text_area("âœï¸ Ø§ÙƒØªØ¨ Ø£Ùˆ Ø§Ù„ØµÙ‚ Ù†Øµ Ø§Ù„Ù…Ù‚Ø§Ù„ Ù‡Ù†Ø§:", height=250)

    if st.button("ğŸ” ØªØµÙ†ÙŠÙ ÙˆØªÙ„Ø®ÙŠØµ Ø§Ù„Ù…Ù‚Ø§Ù„"):
        if not user_input.strip():
            st.warning("âš ï¸ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ù‚Ø¨Ù„ Ø§Ù„ØªØµÙ†ÙŠÙ.")
        else:
            top3_predictions = predict_top3(user_input)
            
            st.success("âœ… Ø£Ø¹Ù„Ù‰ 3 ØªØµÙ†ÙŠÙØ§Øª Ù…Ø­ØªÙ…Ù„Ø©:")
            for label, percent in top3_predictions:
                st.write(f"ğŸ”¹ {label}: {percent}%")

            # ØªØ­Ù„ÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠ Ù„Ù„Ù†Øµ
            st.markdown("---")
            st.info("ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ù†Øµ Ø§Ù„Ù…Ù‚Ø§Ù„:")
            num_words, common_words = analyze_text(user_input)
            st.write(f"- Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {num_words}")
            st.write("- Ø£ÙƒØ«Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØªÙƒØ±Ø§Ø±Ø§Ù‹:")
            for word, count in common_words:
                st.write(f"  â€¢ {word} ({count} Ù…Ø±Ø§Øª)")

            # ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù…Ù‚Ø§Ù„
            st.markdown("---")
            st.success("ğŸ“ ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù…Ù‚Ø§Ù„:")
            try:
                summary = summarize_text(user_input, sentence_count=3)
                st.write(summary if summary else "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù„Ø®Øµ Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ù…Ù‚Ø§Ù„.")
            except Exception as e:
                st.warning("âš ï¸ ØªØ¹Ø°Ø± ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ Ø¨Ø³Ø¨Ø¨ Ù‚ØµØ± Ø§Ù„Ù…Ù‚Ø§Ù„ Ø£Ùˆ Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„.")

# ======== Ø§Ù„ØªØ¨ÙˆÙŠØ¨ Ø§Ù„Ø«Ø§Ù†ÙŠ: Ø­ÙˆÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ========
with tabs[1]:
    show_about()

# ======== ØªØ°ÙŠÙŠÙ„ ========
st.markdown("---")
st.caption("ğŸš€ Ù…Ø´Ø±ÙˆØ¹ Ø·Ù„Ø§Ø¨ÙŠ Ù…Ù‚Ø¯Ù… Ù„Ù…Ù‚Ø±Ø± EMAI 631 - Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ù…Ø­ÙÙˆØ¸Ø© Â© 2025")

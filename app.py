# ========== app.py ==========

import streamlit as st
import requests
import joblib
import re
import os

# ========== ุฅุนุฏุงุฏ ุงูุชูููุงุช ==========
GROQ_API_KEY = os.getenv("GROQ_API_KEY")


# ========== ุชุญููู ุงูููุงุฐุฌ ุงููุฏุฑุจุฉ ==========
tfidf = joblib.load('tfidf_vectorizer.pkl')
label_encoder = joblib.load('label_encoder.pkl')
svm_model = joblib.load('svm_model.pkl')
lr_model = joblib.load('lr_model.pkl')


# ========== ุฏูุงู ูุณุงุนุฏุฉ ==========

# ุชูุธูู ุงููุต
def clean_text(text):
    text = re.sub(r'[^\u0600-\u06FF\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# ุงูุชูุจุค ุจุฃุนูู 3 ุชุตูููุงุช
def predict_top3_with_model(text, model_name):
    cleaned = clean_text(text)
    vectorized = tfidf.transform([cleaned])

    # ุงุฎุชูุงุฑ ุงููููุฐุฌ ุญุณุจ ุงุณู ุงููุณุชุฎุฏู
    if model_name == "Logistic Regression":
        probabilities = lr_model.decision_function(vectorized)
    else:
        probabilities = svm_model.decision_function(vectorized)

    if len(probabilities.shape) == 1:
        probabilities = probabilities.reshape(1, -1)

    top3_idx = probabilities.argsort()[0][-3:][::-1]
    top3_scores = probabilities[0][top3_idx]
    top3_labels = label_encoder.inverse_transform(top3_idx)

    normalized_scores = (top3_scores - top3_scores.min()) / (top3_scores.max() - 1e-6)
    percentages = (normalized_scores * 100).astype(int)

    return list(zip(top3_labels, percentages))


# ุชูุฎูุต ูุงูุชุฑุงุญ ุนููุงู ุนุจุฑ Groq API
def summarize_and_suggest_title(text):
    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {GROQ_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "allam-2-7b",
        "messages": [
            {"role": "system", "content": "ุฃูุช ูุณุงุนุฏ ุฐูู. ุนูุฏูุง ูุตูู ูุต ุทูููุ ูู ุจุชูุฎูุตู ุจุดูู ูุฎุชุตุฑุ ุซู ุงูุชุฑุญ ุนููุงููุง ูุตูุฑูุง ูุฌุฐุงุจูุง ุจุงููุบุฉ ุงูุนุฑุจูุฉ."},
            {"role": "user", "content": f"ูุฐุง ูู ูุต ุงูููุงู:\n\n{text}\n\nุฑุฌุงุกู: 1- ูุฎุต ุงูููุงู ูู ููุฑุฉ ูุตูุฑุฉ. 2- ุงูุชุฑุญ ุนููุงููุง ุฐูููุง ููููุงู."}
        ],
        "temperature": 0.5,
        "max_tokens": 500
    }
    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        reply = response.json()["choices"][0]["message"]["content"].strip()
        return reply
    else:
        return f"โ ุฎุทุฃ ูู ุงูุงุชุตุงู: {response.status_code} - {response.text}"

# ุตูุญุฉ ุญูู ุงููุดุฑูุน
def show_about():
    st.markdown("""
    ## ุญูู ุงููุดุฑูุน ๐ง
    ูุฐุง ุงููุธุงู ูููู ุจุชุตููู ุงูููุงูุงุช ุงูุฅุฎุจุงุฑูุฉ ุงูููุชูุจุฉ ุจุงููุบุฉ ุงูุนุฑุจูุฉ ุจุงุณุชุฎุฏุงู ุงูุฐูุงุก ุงูุงุตุทูุงุนู.
    
    - **ุชุตููู ุงูููุงู** ุจุงุณุชุฎุฏุงู SVM.
    - **ุชูุฎูุต ุงูููุงู ูุงูุชุฑุงุญ ุนููุงู** ุจุงุณุชุฎุฏุงู Groq - Allam 2 7B.
    
    ### 
    ูุดุฑูุน ูููุฑุฑ EMAI 631 โ ูุนุงูุฌุฉ ุงููุบุฉ ุงูุทุจูุนูุฉ (NLP).
    """)

# ========== ุฅุนุฏุงุฏ ูุงุฌูุฉ Streamlit ==========

st.set_page_config(
    page_title="ุชุตููู ุงูุฃุฎุจุงุฑ ุงูุนุฑุจูุฉ ุจุงุณุชุฎุฏุงู ุงูุฐูุงุก ุงูุงุตุทูุงุนู",
    page_icon="๐ฐ",
    layout="centered"
)

# ูุงุฌูุฉ RTL
st.markdown(
    """
    <style>
    body, .stApp {
        direction: rtl;
        text-align: right;
    }
    </style>
    """,
    unsafe_allow_html=True
)

st.title("๐ฐ ุชุตููู ุงูุฃุฎุจุงุฑ ุงูุนุฑุจูุฉ ุจุงุณุชุฎุฏุงู ุงูุฐูุงุก ุงูุงุตุทูุงุนู")

tabs = st.tabs(["๐ฐ ุชุตููู ูุชุญููู ููุงู", "โน๏ธ ุญูู ุงููุดุฑูุน"])




# ======== ุงูุชุจููุจ ุงูุฃูู: ุชุตููู ูุชุญููู ููุงู ========
with tabs[0]:
    st.subheader("๐ ุฃุฏุฎู ูุต ุงูููุงู:")
    user_input = st.text_area("โ๏ธ ุงูุชุจ ุฃู ุงูุตู ูุต ุงูููุงู ููุง:", height=250)

    model_choice = st.selectbox(
    "ุงุฎุชุฑ ูููุฐุฌ ุงูุชุตููู",
    ("SVM", "Logistic Regression")
)

    if st.button("๐ ุชุญููู ุงูููุงู"):
        if not user_input.strip():
            st.warning("โ๏ธ ุงูุฑุฌุงุก ุฅุฏุฎุงู ูุต ูุจู ุงูุชุตููู.")
        else:
            # ุงุณุชุฏุนุงุก ุงูุฏุงูุฉ ูุน ุงุฎุชูุงุฑ ุงููููุฐุฌ
            top3_predictions = predict_top3_with_model(user_input, model_choice)

            st.success("โ ุฃุนูู 3 ุชุตูููุงุช ูุญุชููุฉ:")
            for label, percent in top3_predictions:
                st.write(f"๐น {label}: {percent}%")

            st.markdown("---")
            st.success("๐ ุชูุฎูุต ุงูููุงู ูุงูุชุฑุงุญ ุนููุงู:")
            result = summarize_and_suggest_title(user_input)
            st.write(result)


# ======== ุงูุชุจููุจ ุงูุซุงูู: ุญูู ุงููุดุฑูุน ========
with tabs[1]:
    show_about()

# ======== ุชุฐููู ========
st.markdown("---")
st.caption(" ูุดุฑูุน ููุฏู ูููุฑุฑ EMAI 631 - ุฌููุน ุงูุญููู ูุญููุธุฉ ยฉ 2025")

# ========== app.py ==========

import streamlit as st
import joblib
import re
from collections import Counter
import matplotlib.pyplot as plt

# ========== ุชุญููู ุงูููุงุฐุฌ ุงููุฏุฑุจุฉ ==========
tfidf = joblib.load('tfidf_vectorizer.pkl')
label_encoder = joblib.load('label_encoder.pkl')
svm_model = joblib.load('svm_model.pkl')

# ========== ุฏูุงู ูุณุงุนุฏุฉ ==========

# ุชูุธูู ุงููุต
def clean_text(text):
    text = re.sub(r'[^\u0600-\u06FF\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# ุงูุชูุจุค ุจุฃุนูู 3 ุชุตูููุงุช
def predict_top3(text):
    cleaned = clean_text(text)
    vectorized = tfidf.transform([cleaned])

    try:
        probabilities = svm_model.decision_function(vectorized)
    except:
        probabilities = svm_model.predict_proba(vectorized)

    if len(probabilities.shape) == 1:
        probabilities = probabilities.reshape(1, -1)

    top3_idx = probabilities.argsort()[0][-3:][::-1]
    top3_scores = probabilities[0][top3_idx]
    top3_labels = label_encoder.inverse_transform(top3_idx)

    normalized_scores = (top3_scores - top3_scores.min()) / (top3_scores.max() - top3_scores.min() + 1e-6)
    percentages = (normalized_scores * 100).astype(int)

    return list(zip(top3_labels, percentages))

# ุชุญููู ุงููุต (ุนุฏุฏ ุงููููุงุช + ุงููููุงุช ุงููุชูุฑุฑุฉ)
def analyze_text(text):
    words = clean_text(text).split()
    num_words = len(words)
    most_common = Counter(words).most_common(5)
    return num_words, most_common

# ุชูุณูุฑ ุงููุฑุงุฑ (ุฃูู ุงููููุงุช)
def explain_decision(text, top_n=5):
    vectorized = tfidf.transform([text])
    feature_names = tfidf.get_feature_names_out()
    dense = vectorized.todense().tolist()[0]
    word_scores = list(zip(feature_names, dense))
    sorted_words = sorted(word_scores, key=lambda x: x[1], reverse=True)
    important_words = [word for word, score in sorted_words if score > 0][:top_n]
    return important_words

# ุตูุญุฉ ุญูู ุงููุดุฑูุน
def show_about():
    st.markdown("""
    ## ุญูู ุงููุดุฑูุน ๐ง
    ูุฐุง ุงููุธุงู ูููู ุจุชุตููู ุงูููุงูุงุช ุงูุฅุฎุจุงุฑูุฉ ุงูููุชูุจุฉ ุจุงููุบุฉ ุงูุนุฑุจูุฉ ุฅูู ูุฆุงุช ูุญุฏุฏุฉ ุจุงุณุชุฎุฏุงู ุงูุฐูุงุก ุงูุงุตุทูุงุนู.
    
    - **ูุฌููุนุฉ ุงูุจูุงูุงุช**: SANAD Dataset.
    - **ุงูุชูุซูู ุงููุตู**: TF-IDF Vectorization.
    - **ุงููููุฐุฌ ุงููุณุชุฎุฏู**: Support Vector Machine (SVM).
    - **ููุฒุงุช ุฅุถุงููุฉ**: ุนุฑุถ ุฃูุถู 3 ุชุตูููุงุชุ ุชูุณูุฑ ูุฑุงุฑ ุงูุชุตูููุ ูุชุญููู ูุตู ุจุณูุท.
    
    ### ุฅุนุฏุงุฏ ุงูุทุงูุจ:
    ูุดุฑูุน ูููุฑุฑ EMAI 631 โ ูุนุงูุฌุฉ ุงููุบุฉ ุงูุทุจูุนูุฉ (NLP).
    """)

# ========== ุฅุนุฏุงุฏ ูุงุฌูุฉ Streamlit ==========

st.set_page_config(
    page_title="ุชุตููู ุงูุฃุฎุจุงุฑ ุงูุนุฑุจูุฉ ุจุงุณุชุฎุฏุงู ุงูุฐูุงุก ุงูุงุตุทูุงุนู",
    page_icon="๐ฐ",
    layout="centered"
)

# ูุงุฌูุฉ RTL
st.markdown(
    """
    <style>
    body, .stApp {
        direction: rtl;
        text-align: right;
    }
    </style>
    """,
    unsafe_allow_html=True
)

st.title("๐ฐ ุชุตููู ุงูุฃุฎุจุงุฑ ุงูุนุฑุจูุฉ ุจุงุณุชุฎุฏุงู ุงูุฐูุงุก ุงูุงุตุทูุงุนู")

tabs = st.tabs(["๐ฐ ุชุตููู ููุงู", "โน๏ธ ุญูู ุงููุดุฑูุน"])

# ======== ุงูุชุจููุจ ุงูุฃูู: ุชุตููู ููุงู ========
with tabs[0]:
    st.subheader("๐ ุฃุฏุฎู ูุต ุงูููุงู:")
    user_input = st.text_area("โ๏ธ ุงูุชุจ ุฃู ุงูุตู ูุต ุงูููุงู ููุง:", height=250)

    if st.button("๐ ุชุตููู ูุชุญููู ุงูููุงู"):
        if not user_input.strip():
            st.warning("โ๏ธ ุงูุฑุฌุงุก ุฅุฏุฎุงู ูุต ูุจู ุงูุชุตููู.")
        else:
            # ุชุตููู
            top3_predictions = predict_top3(user_input)
            
            st.success("โ ุฃุนูู 3 ุชุตูููุงุช ูุญุชููุฉ:")
            for label, percent in top3_predictions:
                st.write(f"๐น {label}: {percent}%")

            # ุชุญููู ุฅุถุงูู
            st.markdown("---")
            st.info("๐ ุชุญููู ูุต ุงูููุงู:")
            num_words, common_words = analyze_text(user_input)
            st.write(f"- ุนุฏุฏ ุงููููุงุช: {num_words}")
            st.write("- ุฃูุซุฑ ุงููููุงุช ุชูุฑุงุฑุงู:")
            for word, count in common_words:
                st.write(f"  โข {word} ({count} ูุฑุงุช)")

            # ุชูุณูุฑ ุงููุฑุงุฑ
            st.markdown("---")
            st.success("๐ง ุชูุณูุฑ ูุฑุงุฑ ุงูุชุตููู (ุฃูู ุงููููุงุช ุงููุคุซุฑุฉ):")
            important_words = explain_decision(user_input, top_n=5)
            if important_words:
                st.write(", ".join(important_words))
            else:
                st.write("ูุง ุชูุฌุฏ ูููุงุช ูุคุซุฑุฉ ูุงููุฉ.")

# ======== ุงูุชุจููุจ ุงูุซุงูู: ุญูู ุงููุดุฑูุน ========
with tabs[1]:
    show_about()

# ======== ุชุฐููู ========
st.markdown("---")
st.caption("๐ ูุดุฑูุน ุทูุงุจู ููุฏู ูููุฑุฑ EMAI 631 - ุฌููุน ุงูุญููู ูุญููุธุฉ ยฉ 2025")
